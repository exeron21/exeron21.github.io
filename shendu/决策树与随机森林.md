随机森林和GBDT都是归到Ensemble，他们的底层都是决策树？

## 决策树
- 树型结构决策规则
- 分类问题，对样本：
    - 从根节点出发
    - 根据节点规则决定走哪个子节点
    - 一直走到叶子节点
    - 根据叶子节点的 **输出规则** 输出

- 例子：购买评估
    - 内部节点表示一个特征或属性
    - 叶子节点表示一个类别

- 年龄作为根节点
    - 青年，{否，买}
    - 中年，{买}
    - 老年，{否，买}
- “中年”停止分解，“青年”、“老年”继续分解


## softmax,sigmoid

## 信息熵，香农定理
- 不确定性函数I称为事件的信息量，事件U发生概率p的单调递减函数：
    I(U) = log(1/p) = -log(p)
- 信息熵：一个信息源中，不能仅考虑单一事件发生的不确定性，需要考虑所有可能情况的平均不确定性，为-log(p)的统计平均值E
    H(U) = E[-log(p(ui))] = -sig p(ui)log(p(ui))
- 信息熵是事物不确定性的度量标准
- 决策树中，不仅可用来度量类别类别不确定性，也可以度量包含不同特性的数据样本与类别的不确定性
- 熵越大，不确定性就越大，即混乱程度越大


## 学习算法简介
- 学习算法
    - 给定训练数据，决定树的结构
        - **节点分裂规则**
        - **叶子节点输出规则**
    - 著名算法 CART sklean和Spark上面的实现都是用CART
        - ID3
        - C4.5
            - 连续变量、缺省值、剪枝等

## ID3学习算法
- ID3
    - 输入：离散值（属性）
    - 使用信息增益来学习分裂规则
- 信息熵Entropy
    - S是样例集合
    - p(ui)表示S中第i类样例的比例
- 信息增益
    - 用规则r将样例集合S分为k个子集S1、S2、……Sk
        - |*|表示集合元素个数
- 节点分裂规则：
    - 按属性：k种可能的属性值->k个子集
- 学习问题：挑哪个属性？
    - ID3算法：信息增益最大的！比如数据中有 **年龄段**，**是否学生**，**收入**，**信誉** 这几种属性，那怎么选择属性呢？依次选择信息增益最大的，比如 **年龄段** 的信息增益最大，那就先通过 **年龄段** 来划分，再选择信息增益第二大的进行划分，依次类推。。。

## 信息增益如何计算？
- 用信息熵来度量每种特性不同取值的不确定性
- 设A具有v个不同的值{a1,a2,...av}
- 某一特性A将S划分为v个不同的子集{S1,S2,...Sv}
- 其中Sj包含S中这样一些样本：他们在A上具有值aj，若选A作测试特征，即最优划分特征，那么这些子集就是S节点中生长出来的决策树分支。设Sij是子集Sj中类Cj的样本数。


- 接前面例子：
- 类别标签S划分为两类：买或不买

- 总体S=S1+S2=1024
- 最后


- ID3
    - 创建根节点R
    - 如果当前DataSet中数据是纯的：将R的节点类型标记为当前类型
    - 如果当Attrlist为空：将R的节点类型标记为当前Dataset中，样例最多的那个类型
    - **以上为终止条件**
    - 其余情况：
        - 从attrList中选择属性A
        - 按照属性A所有的不同值Vi，将Dataset分为不同的子集Di，对于每个Di：
            - 创建节点C
            - 如果Di为空，节点C标记为Dataset中，样例个数最多的类型
            - Di不为空，节点C=ID3(Di, attrList - A)
            - 将节点C添加为R的子节点
    - **以上为递归条件**

- ID3缺点？
    - 倾向于挑选属性值较多样本的属性，有些情况可能不会提供太多有价值的信息
        - 贪婪性
        - 奥卡姆剃刀原理：尽量用较少的东西做更多的事情
    - **不适用于连续变量**


## C4.5算法
- 相对于ID3：
    - 克服了用信息增益选择属性时偏向选择取值多的属性的不足(性别：男，女，年龄段：5-10，10-15，15-20，20-25，25-30。。。)
    - 支持连续变量[0-3] [0.5,1.5,2.5]