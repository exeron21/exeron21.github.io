从数学中获取数据


最小二乘法是高中知识？？？

## 数学基础内容与学习路线

- week1:线性代数，被人誉为21世纪的数学，在机器学习当中十分重要，机器学习中很多算法都是用线性代数来表示的，这样会使得数据和算法十分简洁，降低复杂度。
    - 标量、向量与矩阵，矩阵的运算
    - 线性相关与线性无关，矩阵的秩
    - 矩阵的范数与迹
    - 矩阵变换、矩阵分解
    - PCA推导
- week2:微积分，微积分是高等数学的基础，也是机器学习的基础，机器学习中会使用到微分和积分
    - 集合与函数
    - 极限
    - 连续函数和微分
    - 积分
    - 矩阵的求导与Hessian矩阵
- week3:概率论+统计，有人说机器学习就是统计学习方法，机器学习确实是以统计学为基础的
    - 概率论基础
    - 常用概率分布
    - 联合分布与边缘分布
    - 条件概率
    - 期望与方差
- week4:概率论+信息论，交叉熵损失函数是最常用的基于信息论的
    - 极大似然估计与最大后验估计
    - 贝叶斯法则
    - 信息的度量
    - 信息熵、互信息
    - KL散度（相对熵）
- week5:优化方法，比较难。当我们得到了约束条件之后，我们必须使用优化方法得到问题的最优解，在线性回归中，使用最小二乘法得到所有点到直线的最短距离就是一个最常用的优化方法。
    - 最速下降法
    - 梯度下降法
    - 牛顿下降法、拟牛顿法
    - 共轭梯度法
    - adman方法
    - 拉格朗日乘数法

## 学习数学基础的建议
- 重视基础概念和定理
- 不过多纠结具体证明过程
- 遇到问题多与其他人交流
- 两本书：
    - 花书、数学之美第2版



## 标量、向量与矩阵
要求掌握：
- 标量与向量
- 向量运算
- 矩阵定义
- 矩阵运算
- 特殊矩阵


标量与向量
- **向量的元素个数就是维度，如[1,5,8,3,2]就是5维的**
- 标量是一个单独的数，一个用斜体普通小写字母或希腊字母表示，如a
- 向量是一个同时具有大小和方向的几何对象，一般用粗体小写字母来表示向量，小写字母上有一个小箭头，这才是向量的一般表示法$\arrow{a}$。
- **向量**(vector)：一个向量是一列数。这些数是有序排列的。通过次序中的索引，我们可以确定每个单独的数。通常我们赋予向量粗估的小写变量名称，比如 **x**。向量中的元素可以通过带脚标的斜体表示。向量a中的第一个元素是a1,第2个元素是a2...我们也会注明存储在向量中的元素是什么类型的，如果每个元素都属于实数R，并且该向量有n个元素，那么该向量属于实数集R的n次笛卡尔积构成的集合，记为R^n。当需要明确表示向量中的元素时，我们会将元素排列成一个方括号包围的纵列：
[x1]
[x2]
[x3]
...
[xn]
我们可以把向量看作空间中的点，每个元素是不同坐标轴上的坐标。
有时我们需要索引向量中的一些元素。在这种情况下，我们定义一个包含这些元素索引的集合，然后将该集合写在脚标处，比如指定x1,x3,x6，我们定义交叉集合S={1,3,6}，然后写做x_S。我们用符号-表示集合的补集中的索引。比如x_{-1}表示x中除x_1之外的所有元素。x_{-S}表示x中除x1,x3,x6之外的所有元素构成的向量

### 向量的表示
- 向量一般用粗体小写字母或希腊字母表示，如x等（有时也会用箭头来标识），其元素记作x_i
- 向量分类：
    - 行向量
    - 列向量

### 向量的一般属性
- 向量的模：所有元素的平方和再开方
- 向量的范数：||a||1,||a||2
    - 1范数，各元素绝对值的和
    - 2范数，就是向量的模，各元素平方和再开方
    - 无穷范数，所有元素绝对值中取最大

### 向量运算
- 形状不同的向量不能运算
- 向量的加法：等于各元素相加
- 向量的乘积--点积（代数定义）：
    - 点积：两个形状一样的向量，各位置的元素相乘之后求和，a*b还能写成|a*b^T|，其中T表示转置，而||表示行列式。
    - 元素乘/数乘：一个向量和一个标量相乘，等于向量中的各元素与标量相乘。


- 向量的乘积--点积（几何定义）
在欧氏空间中，点积可以直观地定义为：
a*b = |a||b|cos\thta, ||表示向量的模，\thta表示 两个向量之间的角度。

欧氏空间中的向量A在向量B上的标量投影是指：
A_B = |A|cos\thta

- 向量的乘积--点积（高维空间的定义方式）


### 矩阵定义
- 为什么要学习矩阵？ 机器学习基础公式：y = f(x) = xw^T+b ，即可以用来表示线性回归模型，也可以表示神经网络中的前向传播，公式中的w,b,x都是向量或者矩阵。

- 矩阵(matrix)：矩阵是一个二维数组，其中的每一个元素被两个索引（而非一个）所确定。我们通常会赋予矩阵 **粗体的大写变量名称**，比如A（**向量是小写的粗体**，**标量是小写非粗体**）